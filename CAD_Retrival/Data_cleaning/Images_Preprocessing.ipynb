{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Hash: Make new folder for duplicates and copy distinct to One folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "source_folder = \"P:\\CAD_Retrival\\Images\"  # Replace with your image folder path\n",
    "output_folder = \"P:\\CAD_Retrival\\Duplicates\"  # Folder where duplicate folders will be stored\n",
    "distinct_folder = \"P:\\CAD_Retrival\\Distinct\"  # Folder for unique images\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(distinct_folder, exist_ok=True)\n",
    "\n",
    "# Function to compute hash of an image\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        hasher = hashlib.md5()\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            buf = f.read()\n",
    "            hasher.update(buf)\n",
    "        return hasher.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store image hashes and their original paths\n",
    "hash_dict = {}\n",
    "duplicates_dict = {}\n",
    "\n",
    "# Scan folder for images\n",
    "count = 0\n",
    "for root, _, files in os.walk(source_folder):  # Use os.walk() to scan all files\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            img_hash = compute_hash(file_path)\n",
    "            \n",
    "            if img_hash:\n",
    "                if img_hash in hash_dict:\n",
    "                    if img_hash not in duplicates_dict:\n",
    "                        duplicates_dict[img_hash] = []  # Create list for duplicates\n",
    "\n",
    "                    duplicates_dict[img_hash].append(file_path)\n",
    "                else:\n",
    "                    hash_dict[img_hash] = file_path  # Store first occurrence\n",
    "\n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print(f\"Processed {count} images...\")  # Print progress every 500 images\n",
    "\n",
    "# Move distinct images to Distinct_Objects folder\n",
    "for img_hash, original_path in hash_dict.items():\n",
    "    shutil.move(original_path, os.path.join(distinct_folder, os.path.basename(original_path)))\n",
    "\n",
    "# Create folders and move duplicates\n",
    "for img_hash, duplicate_paths in duplicates_dict.items():\n",
    "    distinct_image_name = os.path.basename(hash_dict[img_hash]).split('.')[0]\n",
    "    duplicate_folder = os.path.join(output_folder, distinct_image_name)\n",
    "    \n",
    "    os.makedirs(duplicate_folder, exist_ok=True)  # Create folder for each distinct image\n",
    "\n",
    "    # Move duplicate images into the respective folder\n",
    "    for dup in duplicate_paths:\n",
    "        shutil.move(dup, os.path.join(duplicate_folder, os.path.basename(dup)))\n",
    "\n",
    "print(f\"\\n Processed {count} images.\")\n",
    "print(f\" Moved distinct images to {distinct_folder}.\")\n",
    "print(f\" Moved duplicate images into separate folders in {output_folder}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Move One image from each duplicate folder -> Distict Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "duplicates_folder = \"P:\\CAD_Retrival\\Duplicates\"  # Folder containing duplicate directories\n",
    "distinct_folder = \"P:\\CAD_Retrival\\Distinct\"  # Folder where one image per duplicate set will be copied\n",
    "\n",
    "# Ensure the distinct folder exists\n",
    "os.makedirs(distinct_folder, exist_ok=True)\n",
    "\n",
    "# Process each duplicate folder\n",
    "for folder_name in os.listdir(duplicates_folder):\n",
    "    folder_path = os.path.join(duplicates_folder, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):  # Ensure it's a directory\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        if files:  # Ensure the folder is not empty\n",
    "            first_image = os.path.join(folder_path, files[0])  # Pick the first image\n",
    "            destination_path = os.path.join(distinct_folder, files[0])\n",
    "            \n",
    "            shutil.copy(first_image, destination_path)  # Copy the file\n",
    "            \n",
    "            print(f\"✅ Copied: {first_image} → {destination_path}\")\n",
    "\n",
    "print(f\"\\n✅ Successfully copied one image from each duplicate folder to {distinct_folder}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creat folder of max 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define source and destination directories\n",
    "src = r\"P:\\CAD_Retrival\\Filtered_2_N\"\n",
    "dest = r\"P:\\CAD_Retrival\\b_a_Filtered_2_N_Sets(1000)\"\n",
    "\n",
    "# Create destination directory if it doesn't exist\n",
    "os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "# List all files in the source directory\n",
    "files = os.listdir(src)\n",
    "\n",
    "count = 0\n",
    "foldernum = 1\n",
    "current_folder = os.path.join(dest, f\"Set_{foldernum}\")\n",
    "os.makedirs(current_folder, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(src, file)\n",
    "    # Check if it is a file (you may add additional image file type checks if needed)\n",
    "    if os.path.isfile(file_path):\n",
    "        shutil.copy(file_path, current_folder)\n",
    "        count += 1\n",
    "        if count == 1000:\n",
    "            foldernum += 1\n",
    "            count = 0\n",
    "            current_folder = os.path.join(dest, f\"Set_{foldernum}\")\n",
    "            os.makedirs(current_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DB Scan on Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "BASE_IMAGE_DIR = r\"P:\\CAD_Retrival\\b_a_Filtered_2_N_Sets(1000)\"  # Parent folder containing your sets (e.g., Set_1, Set_2, ...)\n",
    "BASE_TARGET_DIR = r\"P:\\CAD_Retrival\\b_c_Filtered_2_N_Clusters\"          # Parent folder for all clustering results (for clusters)\n",
    "FEATURE_OUTPUT_DIR = r\"P:\\CAD_Retrival\\b_b_Filtered_2_N_Features\"      # Directory where feature files will be stored\n",
    "ALL_NOISE_DIR = r\"P:\\CAD_Retrival\\b_d_Filtered_2_N_All_Noise\"            # Common directory for all noise images from all sets\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 20  # (Not used in this example, but available for future batch processing)\n",
    "\n",
    "# DBSCAN parameters\n",
    "EPSILON = 10     # Adjust based on image similarity\n",
    "MIN_SAMPLES = 2   # Minimum images per cluster\n",
    "\n",
    "# Control for which sets to process (1-indexed)\n",
    "START_SET = 1  # Process starting from this set (inclusive)\n",
    "END_SET = 198  # Process up to this set (inclusive)\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(FEATURE_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_NOISE_DIR, exist_ok=True)\n",
    "\n",
    "# Load VGG16 Model (Feature Extractor)\n",
    "print(\"Loading VGG16 model...\")\n",
    "model = models.vgg16(pretrained=True).to(DEVICE)\n",
    "model.classifier = model.classifier[:-1]  # Remove the last classification layer\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\\n\")\n",
    "\n",
    "# Image Preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_features(image_paths):\n",
    "    \"\"\"Extracts features from images using VGG16 on GPU.\"\"\"\n",
    "    print(\"Extracting features...\")\n",
    "    all_features = []\n",
    "    valid_image_paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "                torch.cuda.synchronize()  # Ensure GPU work is completed\n",
    "                features = model(img_tensor).cpu().numpy()\n",
    "                all_features.append(features.flatten())\n",
    "                valid_image_paths.append(img_path)\n",
    "                print(f\"Processed {i+1}/{len(image_paths)} images\", end=\"\\r\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    print(\"\\nFeature extraction completed.\\n\")\n",
    "    return np.array(all_features), valid_image_paths\n",
    "\n",
    "def perform_clustering(features_array):\n",
    "    \"\"\"Clusters features using DBSCAN and returns the labels.\"\"\"\n",
    "    print(\"Performing DBSCAN clustering...\")\n",
    "    dbscan = DBSCAN(eps=EPSILON, min_samples=MIN_SAMPLES, metric='euclidean')\n",
    "    labels = dbscan.fit_predict(features_array)\n",
    "    print(\"DBSCAN clustering completed.\\n\")\n",
    "    return labels\n",
    "\n",
    "def organize_images(image_paths, labels, cluster_target_dir, noise_target_dir, cluster_prefix):\n",
    "    \"\"\"\n",
    "    Sorts images into cluster folders under cluster_target_dir.\n",
    "    Images labeled as noise (cluster == -1) are stored in noise_target_dir.\n",
    "    Cluster folders are named using the cluster_prefix (e.g., \"S1_cluster0\").\n",
    "    \"\"\"\n",
    "    print(\"Organizing images into cluster folders...\\n\")\n",
    "    os.makedirs(cluster_target_dir, exist_ok=True)\n",
    "    os.makedirs(noise_target_dir, exist_ok=True)\n",
    "\n",
    "    for i, (path, cluster) in enumerate(zip(image_paths, labels)):\n",
    "        if cluster == -1:\n",
    "            dest_dir = noise_target_dir\n",
    "        else:\n",
    "            dest_dir = os.path.join(cluster_target_dir, f\"{cluster_prefix}_cluster{cluster}\")\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy2(path, dest_dir)\n",
    "        print(f\"Image {i+1}/{len(image_paths)} moved to {'noise' if cluster == -1 else f'{cluster_prefix}_cluster{cluster}'}\", end=\"\\r\")\n",
    "    print(\"\\nImages successfully organized.\\n\")\n",
    "\n",
    "def extract_set_number(folder_name):\n",
    "    \"\"\"Extracts the numeric part from a folder name formatted as 'Set_<number>'.\"\"\"\n",
    "    try:\n",
    "        return int(folder_name.split('_')[1])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting number from folder '{folder_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Get list of set folders from BASE_IMAGE_DIR.\n",
    "    all_set_folders = [d for d in os.listdir(BASE_IMAGE_DIR) if os.path.isdir(os.path.join(BASE_IMAGE_DIR, d))]\n",
    "    \n",
    "    # Filter and sort folders based on the numeric part of their names.\n",
    "    valid_set_folders = []\n",
    "    for folder in all_set_folders:\n",
    "        num = extract_set_number(folder)\n",
    "        if num is not None and START_SET <= num <= END_SET:\n",
    "            valid_set_folders.append(folder)\n",
    "    valid_set_folders = sorted(valid_set_folders, key=lambda x: extract_set_number(x))\n",
    "    \n",
    "    print(f\"Processing the following set folders: {valid_set_folders}\\n\")\n",
    "\n",
    "    for set_folder in valid_set_folders:\n",
    "        print(f\"Processing set: {set_folder}\")\n",
    "        IMAGE_DIR = os.path.join(BASE_IMAGE_DIR, set_folder)\n",
    "        # Cluster output for this set is stored in a folder like \"Set_1_output\"\n",
    "        cluster_target_dir = os.path.join(BASE_TARGET_DIR, f\"{set_folder}_output\")\n",
    "        # Noise images from all sets are stored in ALL_NOISE_DIR under a subfolder named as \"noise_setX\"\n",
    "        set_num = extract_set_number(set_folder)\n",
    "        if set_num is not None:\n",
    "            noise_subfolder = f\"noise_set{set_num}\"\n",
    "            cluster_prefix = f\"S{set_num}\"\n",
    "        else:\n",
    "            noise_subfolder = f\"noise_{set_folder}\"\n",
    "            cluster_prefix = set_folder\n",
    "        noise_target_dir = os.path.join(ALL_NOISE_DIR, noise_subfolder)\n",
    "        feature_file_path = os.path.join(FEATURE_OUTPUT_DIR, f\"{set_folder}_features.npz\")\n",
    "        \n",
    "        # Check if cluster or noise folders already exist and have content\n",
    "        if ((os.path.exists(cluster_target_dir) and os.listdir(cluster_target_dir)) or \n",
    "            (os.path.exists(noise_target_dir) and os.listdir(noise_target_dir))):\n",
    "            answer = input(f\"Cluster and/or noise folders for {set_folder} already exist. Do you want to delete them and replace with new clusters? (y/n): \")\n",
    "            if answer.lower() == 'y':\n",
    "                if os.path.exists(cluster_target_dir):\n",
    "                    shutil.rmtree(cluster_target_dir)\n",
    "                if os.path.exists(noise_target_dir):\n",
    "                    shutil.rmtree(noise_target_dir)\n",
    "            else:\n",
    "                print(f\"Skipping processing for {set_folder}.\\n\")\n",
    "                continue\n",
    "        \n",
    "        # Collect all image paths in the current set\n",
    "        image_paths = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if os.path.isfile(os.path.join(IMAGE_DIR, f))]\n",
    "        print(f\"Found {len(image_paths)} images in {set_folder}.\\n\")\n",
    "        \n",
    "        # Check if feature file already exists\n",
    "        if os.path.exists(feature_file_path):\n",
    "            print(f\"Feature file for {set_folder} exists. Loading features from file...\\n\")\n",
    "            data = np.load(feature_file_path, allow_pickle=True)\n",
    "            features_array = data[\"features\"]\n",
    "            valid_image_paths = list(data[\"image_paths\"])\n",
    "            image_names = list(data[\"image_names\"])\n",
    "            # Perform clustering on loaded features\n",
    "            labels = perform_clustering(features_array)\n",
    "        else:\n",
    "            # Extract features if feature file does not exist\n",
    "            features_array, valid_image_paths = extract_features(image_paths)\n",
    "            if features_array.size == 0:\n",
    "                print(f\"No valid features extracted for {set_folder}. Skipping clustering.\\n\")\n",
    "                continue\n",
    "            labels = perform_clustering(features_array)\n",
    "            image_names = [os.path.basename(path) for path in valid_image_paths]\n",
    "            # Save the extracted features, image names, and image paths to a file\n",
    "            np.savez_compressed(feature_file_path,\n",
    "                                features=features_array,\n",
    "                                image_names=image_names,\n",
    "                                image_paths=valid_image_paths)\n",
    "            print(f\"Saved features to {feature_file_path}\\n\")\n",
    "        \n",
    "        if len(labels) > 0:\n",
    "            organize_images(valid_image_paths, labels, cluster_target_dir, noise_target_dir, cluster_prefix)\n",
    "            print(f\"Completed clustering for {set_folder}.\\n\")\n",
    "        else:\n",
    "            print(f\"No clusters generated for {set_folder}.\\n\")\n",
    "\n",
    "    print(f\"Total execution time: {time.time() - total_start_time:.2f} seconds.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_cluster_summary_all_sets(base_cluster_dir, base_noise_dir):\n",
    "    \"\"\"\n",
    "    Aggregates and visualizes summary statistics for each set.\n",
    "    \n",
    "    Assumptions:\n",
    "      - Each set's cluster output is in a subdirectory of base_cluster_dir (e.g., \"Set_156_output\").\n",
    "      - Cluster folders inside a set are named like \"S156_clusterX\".\n",
    "      - Noise images for a set are stored in base_noise_dir in a subfolder named \"noise_set156\".\n",
    "    \n",
    "    Returns a DataFrame with the following columns:\n",
    "      - set: The set number.\n",
    "      - num_clusters: The number of cluster subfolders.\n",
    "      - total_cluster_images: The total number of images in all clusters.\n",
    "      - noise_count: The number of images in the noise folder.\n",
    "      - avg_cluster_size: The average number of images per cluster.\n",
    "      - median_cluster_size: The median number of images per cluster.\n",
    "    \n",
    "    Also produces a bar chart comparing total cluster images and noise images per set.\n",
    "    \"\"\"\n",
    "    # Get a list of set folders from base_cluster_dir\n",
    "    set_folders = [d for d in os.listdir(base_cluster_dir) if os.path.isdir(os.path.join(base_cluster_dir, d))]\n",
    "    summary_list = []\n",
    "    \n",
    "    for set_folder in set_folders:\n",
    "        set_path = os.path.join(base_cluster_dir, set_folder)\n",
    "        # Expecting folder names like \"Set_156_output\"\n",
    "        try:\n",
    "            parts = set_folder.split('_')\n",
    "            set_num = int(parts[1])\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting set number from {set_folder}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # For each cluster folder inside this set folder, count the number of image files\n",
    "        cluster_counts = []\n",
    "        for subfolder in os.listdir(set_path):\n",
    "            subfolder_path = os.path.join(set_path, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                count = len([f for f in os.listdir(subfolder_path) \n",
    "                             if os.path.isfile(os.path.join(subfolder_path, f))])\n",
    "                cluster_counts.append(count)\n",
    "        num_clusters = len(cluster_counts)\n",
    "        total_cluster_images = sum(cluster_counts) if cluster_counts else 0\n",
    "        avg_cluster_size = np.mean(cluster_counts) if cluster_counts else 0\n",
    "        median_cluster_size = np.median(cluster_counts) if cluster_counts else 0\n",
    "        \n",
    "        # Get noise count from the corresponding noise folder in base_noise_dir (e.g., \"noise_set156\")\n",
    "        noise_folder = os.path.join(base_noise_dir, f\"noise_set{set_num}\")\n",
    "        if os.path.exists(noise_folder):\n",
    "            noise_count = len([f for f in os.listdir(noise_folder) \n",
    "                               if os.path.isfile(os.path.join(noise_folder, f))])\n",
    "        else:\n",
    "            noise_count = 0\n",
    "        \n",
    "        summary_list.append({\n",
    "            \"set\": set_num,\n",
    "            \"num_clusters\": num_clusters,\n",
    "            \"total_cluster_images\": total_cluster_images,\n",
    "            \"noise_count\": noise_count,\n",
    "            \"avg_cluster_size\": avg_cluster_size,\n",
    "            \"median_cluster_size\": median_cluster_size\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary_list)\n",
    "    df.sort_values(\"set\", inplace=True)\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    display(df)\n",
    "    \n",
    "    # Plotting a bar chart for total cluster images vs. noise images per set\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x_labels = df[\"set\"].astype(str)\n",
    "    plt.bar(x_labels, df[\"total_cluster_images\"], label=\"Cluster Images\", alpha=0.7)\n",
    "    plt.bar(x_labels, df[\"noise_count\"], label=\"Noise Images\", alpha=0.7)\n",
    "    plt.xlabel(\"Set Number\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.title(\"Cluster Images vs. Noise Images per Set\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = visualize_cluster_summary_all_sets(r\"P:\\CAD_Retrival\\c_Clusters\", r\"P:\\CAD_Retrival\\d_All_Noise\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Detailed Information about Sets and Cluster -> csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've run the visualization function and stored the DataFrame in 'df_summary'\n",
    "df_summary.to_csv(\"cluster_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_detailed_cluster_counts(base_cluster_dir, base_noise_dir):\n",
    "    \"\"\"\n",
    "    Creates a detailed DataFrame with the number of images in each cluster (and in the noise folder)\n",
    "    for each set.\n",
    "    \n",
    "    Assumptions:\n",
    "      - Each set's cluster output is stored in a subdirectory of base_cluster_dir,\n",
    "        with names like \"Set_156_output\".\n",
    "      - Inside each set folder, cluster subdirectories are named using a pattern like \"S156_cluster0\", \"S156_cluster1\", etc.\n",
    "      - The corresponding noise folder for a set is stored in base_noise_dir, with a name like \"noise_set156\".\n",
    "    \n",
    "    Returns:\n",
    "      A Pandas DataFrame with columns:\n",
    "        - set: The set number.\n",
    "        - cluster: The cluster name (e.g., \"S156_cluster0\", or \"noise\" for the noise folder).\n",
    "        - num_images: The number of images in that cluster.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Get list of set folders in base_cluster_dir\n",
    "    set_folders = [d for d in os.listdir(base_cluster_dir) if os.path.isdir(os.path.join(base_cluster_dir, d))]\n",
    "    \n",
    "    # Sort the set folders by the numeric part (assumes format like \"Set_156_output\")\n",
    "    def extract_set_num(folder_name):\n",
    "        try:\n",
    "            # Example folder: \"Set_156_output\"\n",
    "            parts = folder_name.split('_')\n",
    "            return int(parts[1])\n",
    "        except Exception:\n",
    "            return -1  # fallback, so these come first\n",
    "    set_folders = sorted(set_folders, key=extract_set_num)\n",
    "    \n",
    "    for set_folder in set_folders:\n",
    "        set_num = extract_set_num(set_folder)\n",
    "        if set_num == -1:\n",
    "            continue\n",
    "        set_path = os.path.join(base_cluster_dir, set_folder)\n",
    "        \n",
    "        # Process each cluster folder inside this set folder\n",
    "        for cluster_folder in os.listdir(set_path):\n",
    "            cluster_folder_path = os.path.join(set_path, cluster_folder)\n",
    "            if os.path.isdir(cluster_folder_path):\n",
    "                count = len([f for f in os.listdir(cluster_folder_path) \n",
    "                             if os.path.isfile(os.path.join(cluster_folder_path, f))])\n",
    "                records.append({\"set\": set_num, \"cluster\": cluster_folder, \"num_images\": count})\n",
    "        \n",
    "        # Process the noise folder for this set\n",
    "        noise_folder = os.path.join(base_noise_dir, f\"noise_set{set_num}\")\n",
    "        if os.path.exists(noise_folder):\n",
    "            noise_count = len([f for f in os.listdir(noise_folder)\n",
    "                               if os.path.isfile(os.path.join(noise_folder, f))])\n",
    "        else:\n",
    "            noise_count = 0\n",
    "        records.append({\"set\": set_num, \"cluster\": \"noise\", \"num_images\": noise_count})\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df.sort_values([\"set\", \"cluster\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "base_cluster_dir = r\"P:\\CAD_Retrival\\c_Clusters\"\n",
    "base_noise_dir = r\"P:\\CAD_Retrival\\d_All_Noise\"\n",
    "\n",
    "df_detailed = get_detailed_cluster_counts(base_cluster_dir, base_noise_dir)\n",
    "display(df_detailed)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_detailed.to_csv(\"detailed_cluster_counts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Moving a images from each cluster of each sets and creating meta data(set name , imagename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------- Configuration -----------------\n",
    "BASE_IMAGE_DIR = r\"P:\\CAD_Retrival\\a_Set(1000_images)\"  # Folder with original sets (e.g., Set_156, Set_157, …)\n",
    "BASE_TARGET_DIR = r\"P:\\CAD_Retrival\\a_c_Filtered_2_C_Clusters\"         # Folder containing each set's cluster output (e.g., Set_156_output, etc.)\n",
    "FEATURE_OUTPUT_DIR = r\"P:\\CAD_Retrival\\a_b_Filtered_2_C_Features\"       # Folder where individual feature files are stored\n",
    "DEST_FOLDER = r\"P:\\CAD_Retrival\\Filtered_3_C\"      # Single destination folder for representative images\n",
    "OUTPUT_META_FILE = os.path.join(FEATURE_OUTPUT_DIR, \"f_C_Filtered_DBscan1.npz\")\n",
    "START_SET = 1    # Process sets starting from this number (inclusive)\n",
    "END_SET = 359      # Process sets up to this number (inclusive)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------- Model Setup (VGG16 example) -----------------\n",
    "print(\"Loading VGG16 model...\")\n",
    "model = models.vgg16(pretrained=True).to(DEVICE)\n",
    "model.classifier = model.classifier[:-1]  # Remove final classification layer (output feature size = 4096)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\\n\")\n",
    "\n",
    "# ----------------- Preprocessing -----------------\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ----------------- Helper Functions -----------------\n",
    "\n",
    "def extract_set_number(folder_name):\n",
    "    \"\"\"Extracts the numeric part from a folder name (e.g., 'Set_156' or 'Set_156_output').\"\"\"\n",
    "    try:\n",
    "        return int(folder_name.split('_')[1])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting number from folder '{folder_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "def copy_representative_images_and_save_metadata(base_cluster_dir, feature_output_dir, dest_folder, output_meta_file, start_set, end_set):\n",
    "    \"\"\"\n",
    "    For each set folder in base_cluster_dir (e.g., \"Set_156_output\"), this function:\n",
    "      - Loads the corresponding feature file from feature_output_dir (e.g., \"Set_156_features.npz\").\n",
    "      - Iterates over each non-noise cluster subfolder.\n",
    "      - Copies one representative image (the first image found) from each cluster to dest_folder.\n",
    "      - Retrieves its metadata by matching the image filename in the feature file.\n",
    "      - Accumulates only the set name and image name.\n",
    "    Finally, it saves all metadata into a single NPZ file with keys:\n",
    "      \"sets\" and \"image_names\".\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    meta_list = []  # List of dicts with keys: \"set\" and \"image_name\"\n",
    "\n",
    "    # List valid set folders in the cluster output directory.\n",
    "    all_set_folders = [d for d in os.listdir(base_cluster_dir) if os.path.isdir(os.path.join(base_cluster_dir, d))]\n",
    "    valid_set_folders = []\n",
    "    for folder in all_set_folders:\n",
    "        num = extract_set_number(folder)\n",
    "        if num is not None and start_set <= num <= end_set:\n",
    "            valid_set_folders.append(folder)\n",
    "    valid_set_folders = sorted(valid_set_folders, key=lambda x: extract_set_number(x))\n",
    "    \n",
    "    for set_folder in valid_set_folders:\n",
    "        print(f\"Processing set: {set_folder}\")\n",
    "        set_num = extract_set_number(set_folder)\n",
    "        set_cluster_dir = os.path.join(base_cluster_dir, set_folder)\n",
    "        # Assume corresponding feature file is named \"Set_<num>_features.npz\"\n",
    "        feat_file = os.path.join(feature_output_dir, f\"Set_{set_num}_features.npz\")\n",
    "        if not os.path.exists(feat_file):\n",
    "            print(f\"Feature file for {set_folder} not found. Skipping.\")\n",
    "            continue\n",
    "        data = np.load(feat_file, allow_pickle=True)\n",
    "        # We only need the image names (filenames only)\n",
    "        image_names = list(data[\"image_names\"])\n",
    "        \n",
    "        # Process each non-noise cluster folder (skip folders with \"noise\")\n",
    "        cluster_folders = [d for d in os.listdir(set_cluster_dir)\n",
    "                           if os.path.isdir(os.path.join(set_cluster_dir, d)) and \"noise\" not in d.lower()]\n",
    "        for cl_folder in cluster_folders:\n",
    "            cluster_path = os.path.join(set_cluster_dir, cl_folder)\n",
    "            images = [f for f in os.listdir(cluster_path) if os.path.isfile(os.path.join(cluster_path, f))]\n",
    "            if not images:\n",
    "                continue\n",
    "            rep_img_name = images[0]  # Use the first image as representative.\n",
    "            try:\n",
    "                idx = image_names.index(rep_img_name)\n",
    "            except ValueError:\n",
    "                print(f\"Representative image {rep_img_name} not found in feature file for {set_folder}.\")\n",
    "                continue\n",
    "            # Copy representative image to dest_folder (without renaming)\n",
    "            src_img = os.path.join(cluster_path, rep_img_name)\n",
    "            dest_img = os.path.join(dest_folder, rep_img_name)\n",
    "            shutil.copy2(src_img, dest_img)\n",
    "            print(f\"Copied {src_img} to {dest_img}\")\n",
    "            # Append metadata: save only the set folder name and image name\n",
    "            meta_list.append({\n",
    "                \"set\": set_folder,\n",
    "                \"image_name\": rep_img_name\n",
    "            })\n",
    "    \n",
    "    # Save the accumulated metadata into a single NPZ file.\n",
    "    if meta_list:\n",
    "        sets_arr = np.array([d[\"set\"] for d in meta_list])\n",
    "        image_names_arr = np.array([d[\"image_name\"] for d in meta_list])\n",
    "        np.savez_compressed(output_meta_file,\n",
    "                            sets=sets_arr,\n",
    "                            image_names=image_names_arr)\n",
    "        print(f\"\\nSaved metadata to {output_meta_file}\")\n",
    "    else:\n",
    "        print(\"No metadata collected.\")\n",
    "\n",
    "# ----------------- Main Execution -----------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_start_time = time.time()\n",
    "    copy_representative_images_and_save_metadata(\n",
    "        base_cluster_dir=BASE_TARGET_DIR,\n",
    "        feature_output_dir=FEATURE_OUTPUT_DIR,\n",
    "        dest_folder=DEST_FOLDER,\n",
    "        output_meta_file=OUTPUT_META_FILE,\n",
    "        start_set=START_SET,\n",
    "        end_set=END_SET\n",
    "    )\n",
    "    print(f\"\\nTotal execution time: {time.time() - total_start_time:.2f} seconds.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Copy noise images to one file and meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "def copy_all_noise_images_and_save_metadata(noise_base_dir, dest_folder, output_meta_file):\n",
    "    \"\"\"\n",
    "    Copies all images from each noise folder in noise_base_dir to a single destination folder (dest_folder)\n",
    "    and saves metadata for each copied image. The metadata contains:\n",
    "      - \"set\": the name of the noise folder (e.g., \"noise_set156\")\n",
    "      - \"image_name\": the final image filename in dest_folder\n",
    "\n",
    "    If a filename conflict occurs in dest_folder, a counter is appended to the filename to avoid overwriting.\n",
    "\n",
    "    Parameters:\n",
    "      noise_base_dir (str): Base directory containing noise folders (e.g., \"P:\\CAD_Retrival\\d_All_Noise\")\n",
    "      dest_folder (str): Destination folder where all noise images will be copied.\n",
    "      output_meta_file (str): Path to the NPZ file where metadata will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    meta_list = []  # To accumulate metadata for each copied image\n",
    "\n",
    "    # List all noise folders in the noise base directory.\n",
    "    noise_folders = [d for d in os.listdir(noise_base_dir) if os.path.isdir(os.path.join(noise_base_dir, d))]\n",
    "    \n",
    "    for noise_folder in noise_folders:\n",
    "        folder_path = os.path.join(noise_base_dir, noise_folder)\n",
    "        # List all image files in the current noise folder.\n",
    "        image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        for file_name in image_files:\n",
    "            src_path = os.path.join(folder_path, file_name)\n",
    "            dest_path = os.path.join(dest_folder, file_name)\n",
    "            # If a file with the same name exists, append a counter to avoid overwriting.\n",
    "            if os.path.exists(dest_path):\n",
    "                base, ext = os.path.splitext(file_name)\n",
    "                counter = 1\n",
    "                new_name = f\"{base}_{counter}{ext}\"\n",
    "                dest_path = os.path.join(dest_folder, new_name)\n",
    "                while os.path.exists(dest_path):\n",
    "                    counter += 1\n",
    "                    new_name = f\"{base}_{counter}{ext}\"\n",
    "                    dest_path = os.path.join(dest_folder, new_name)\n",
    "                final_name = new_name\n",
    "            else:\n",
    "                final_name = file_name\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "            print(f\"Copied {src_path} to {dest_path}\")\n",
    "            meta_list.append({\n",
    "                \"set\": noise_folder,\n",
    "                \"image_name\": final_name\n",
    "            })\n",
    "    \n",
    "    if meta_list:\n",
    "        sets_arr = np.array([d[\"set\"] for d in meta_list])\n",
    "        image_names_arr = np.array([d[\"image_name\"] for d in meta_list])\n",
    "        np.savez_compressed(output_meta_file, sets=sets_arr, image_names=image_names_arr)\n",
    "        print(f\"\\nSaved metadata to {output_meta_file}\")\n",
    "    else:\n",
    "        print(\"No noise images found to copy.\")\n",
    "\n",
    "# ----------------- Example Usage -----------------\n",
    "# Adjust these paths to your environment:\n",
    "copy_all_noise_images_and_save_metadata(\n",
    "    noise_base_dir=r\"P:\\CAD_Retrival\\a_d_Filtered_2_C_All_Noise\",\n",
    "    dest_folder=r\"P:\\CAD_Retrival\\Filtered_3_N\",\n",
    "    output_meta_file=r\"P:\\CAD_Retrival\\a_b_Filtered_2_C_Features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def combine_feature_files(feature_dir, output_file):\n",
    "    \"\"\"\n",
    "    Combines all NPZ feature files in the given directory into a single NPZ file.\n",
    "    \n",
    "    Each NPZ file is expected to contain:\n",
    "      - 'features': a numpy array of shape (num_images, feature_dim)\n",
    "      - 'image_names': a list (or array) of image filenames (strings)\n",
    "      - Optionally, 'full_image_paths': a list of full image paths.\n",
    "    \n",
    "    The function loads all NPZ files ending with '_features.npz' in feature_dir,\n",
    "    concatenates the arrays and lists, and saves the combined data into output_file.\n",
    "    \n",
    "    Parameters:\n",
    "      feature_dir (str): Directory containing individual NPZ feature files.\n",
    "      output_file (str): Path where the combined NPZ file will be saved.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_image_names = []\n",
    "    all_full_paths = []\n",
    "    \n",
    "    # List all NPZ files in feature_dir that follow the naming pattern\n",
    "    for fname in os.listdir(feature_dir):\n",
    "        if fname.endswith(\"_features.npz\"):\n",
    "            fpath = os.path.join(feature_dir, fname)\n",
    "            print(f\"Loading features from: {fpath}\")\n",
    "            data = np.load(fpath, allow_pickle=True)\n",
    "            features = data[\"features\"]\n",
    "            image_names = list(data[\"image_names\"])\n",
    "            # If full image paths exist, load them; otherwise, use None for each image.\n",
    "            if \"full_image_paths\" in data:\n",
    "                full_paths = list(data[\"full_image_paths\"])\n",
    "            else:\n",
    "                full_paths = [None] * len(image_names)\n",
    "            \n",
    "            all_features.append(features)\n",
    "            all_image_names.extend(image_names)\n",
    "            all_full_paths.extend(full_paths)\n",
    "    \n",
    "    if all_features:\n",
    "        combined_features = np.concatenate(all_features, axis=0)\n",
    "    else:\n",
    "        combined_features = np.array([])\n",
    "    \n",
    "    np.savez_compressed(output_file,\n",
    "                        features=combined_features,\n",
    "                        image_names=all_image_names,\n",
    "                        full_image_paths=all_full_paths)\n",
    "    print(f\"Combined feature file saved to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "feature_dir = r\"P:\\CAD_Retrival\\b_Features\"\n",
    "output_file = os.path.join(feature_dir, \"combined_features.npz\")\n",
    "combine_feature_files(feature_dir, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device to GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Define a custom module to extract features using VGG16\n",
    "class VGG16Features(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Features, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True).to(device)\n",
    "        self.features = vgg16.features\n",
    "        self.avgpool = vgg16.avgpool\n",
    "        # Use classifier up to the second-to-last layer (4096 features)\n",
    "        self.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the feature extractor and move to GPU\n",
    "model = VGG16Features().to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define transformations for PNG images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),           # Resize to VGG16 input size\n",
    "    transforms.ToTensor(),                   # Convert to tensor\n",
    "    transforms.Normalize(                    # Normalize for VGG16\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Custom Dataset class for loading images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Load PNG and convert to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# Specify your image directory\n",
    "image_dir = \"P:\\CAD_Retrival\\Testing3\"  # Replace with your directory\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Create dataset and dataloader for batch processing\n",
    "dataset = ImageDataset(image_paths, transform=transform)\n",
    "if __name__ == '__main__':\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    # Your code here\n",
    "\n",
    "    # Extract features in batches on GPU\n",
    "    features = []\n",
    "    for batch_images, _ in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "        batch_images = batch_images.to(device)\n",
    "        with torch.no_grad():  # No gradient computation\n",
    "            batch_features = model(batch_images)\n",
    "        features.append(batch_features.cpu())  # Move to CPU to save GPU memory\n",
    "    features = torch.cat(features, dim=0)  # Concatenate all features\n",
    "\n",
    "    # Normalize features for similarity computation\n",
    "    features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "    # Convert features to NumPy array for FAISS\n",
    "    features_np = features.numpy()\n",
    "\n",
    "    # FAISS GPU setup\n",
    "    res = faiss.StandardGpuResources()  # Use GPU resources\n",
    "    index = faiss.IndexFlatIP(4096)  # 4096 dimensions from VGG16\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, index)  # Move index to GPU\n",
    "\n",
    "    # Add features to GPU index\n",
    "    gpu_index.add(features_np)\n",
    "\n",
    "    # Perform range search to find similar images\n",
    "    threshold = 0.9  # Similarity threshold (0 to 1)\n",
    "    lim, D, I = gpu_index.range_search(x=features_np, thresh=threshold)\n",
    "\n",
    "    # Union-find to group similar images\n",
    "    parent = list(range(len(image_paths)))\n",
    "\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])  # Path compression\n",
    "        return parent[x]\n",
    "\n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py  # Union operation\n",
    "\n",
    "    # Group similar images\n",
    "    for i in range(len(image_paths)):\n",
    "        start = lim[i]\n",
    "        end = lim[i + 1]\n",
    "        for j in I[start:end]:\n",
    "            if j != i:  # Skip self-similarity\n",
    "                union(i, j)\n",
    "\n",
    "    # Identify unique groups\n",
    "    groups = {}\n",
    "    for i in range(len(image_paths)):\n",
    "        root = find(i)\n",
    "        if root not in groups:\n",
    "            groups[root] = []\n",
    "        groups[root].append(image_paths[i])\n",
    "\n",
    "    # Create subfolders and copy similar images\n",
    "    similar_dir = \"P:\\CAD_Retrival\\Testing3(Output)\"\n",
    "    os.makedirs(similar_dir, exist_ok=True)\n",
    "    group_counter = 1\n",
    "    for root, img_list in groups.items():\n",
    "        if len(img_list) > 1:  # Only copy groups with multiple images\n",
    "            subfolder = os.path.join(similar_dir, f'group_{group_counter}')\n",
    "            os.makedirs(subfolder, exist_ok=True)\n",
    "            for img_path in img_list:\n",
    "                shutil.copy(img_path, subfolder)  # Copy, not move\n",
    "            group_counter += 1\n",
    "\n",
    "    print(f\"Done! Similar images are copied to subfolders in '{similar_dir}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 0 PNG files in P:\\CAD_Retrival\\Testing3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No image files found in P:\\CAD_Retrival\\Testing3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_folder_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m PNG files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_folder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m features, image_names \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Validate features\u001b[39;00m\n\u001b[0;32m    138\u001b[0m features_np \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m, in \u001b[0;36mextract_features_batch\u001b[1;34m(image_paths, model, transform, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_features_batch\u001b[39m(image_paths, model, transform, device):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo image files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_folder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ImageDataset(image_paths, transform)\n\u001b[0;32m     57\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: No image files found in P:\\CAD_Retrival\\Testing3"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ResNet50 feature extractor\n",
    "class ResNet50Features(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50Features, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True).to(device)\n",
    "        self.features = torch.nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(\"Image not loaded\")\n",
    "            else:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = Image.fromarray(img)\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                return img, os.path.basename(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            return None, os.path.basename(img_path)\n",
    "\n",
    "# Batch feature extraction\n",
    "def extract_features_batch(image_paths, model, transform, device):\n",
    "    if not image_paths:\n",
    "        raise ValueError(f\"No image files found in {image_folder_path}\")\n",
    "    dataset = ImageDataset(image_paths, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    features = []\n",
    "    image_names = []\n",
    "    model.eval()\n",
    "    for batch_images, batch_names in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "        valid_images = [img for img in batch_images if img is not None]\n",
    "        valid_names = [name for img, name in zip(batch_images, batch_names) if img is not None]\n",
    "        if not valid_images:\n",
    "            print(f\"Skipping batch: No valid images\")\n",
    "            continue\n",
    "        batch_tensor = torch.stack(valid_images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_features = model(batch_tensor)\n",
    "        features.append(batch_features.cpu())\n",
    "        image_names.extend(valid_names)\n",
    "    if not features:\n",
    "        raise ValueError(\"No features extracted. Check image files and paths.\")\n",
    "    return torch.cat(features, dim=0), image_names\n",
    "\n",
    "# FAISS index\n",
    "def build_faiss_index(feature_vectors):\n",
    "    d = feature_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    print(\"Adding vectors to FAISS index...\")\n",
    "    index.add(feature_vectors)\n",
    "    print(\"FAISS IndexFlatL2 built.\")\n",
    "    return index\n",
    "\n",
    "# Find similar images\n",
    "def find_similar_images(index, feature_vectors, image_names, k=10, distance_threshold=0.5):\n",
    "    similar_image_folders = {}\n",
    "    for i in range(len(image_names)):\n",
    "        query_vector = np.expand_dims(feature_vectors[i], axis=0)\n",
    "        distances, indices = index.search(query_vector, k + 1)\n",
    "        similar_images = []\n",
    "        for j in range(1, len(indices[0])):\n",
    "            neighbor_idx = indices[0][j]\n",
    "            distance = distances[0][j]\n",
    "            if distance < distance_threshold:\n",
    "                similar_images.append(image_names[neighbor_idx])\n",
    "        if similar_images:\n",
    "            similar_image_folders[image_names[i]] = similar_images\n",
    "            print(f\"Found {len(similar_images)} similar images for {image_names[i]}\")\n",
    "    return similar_image_folders\n",
    "\n",
    "# Copy images\n",
    "def copy_similar_images_to_folders(similar_image_folders, image_folder_path, output_base_folder):\n",
    "    os.makedirs(output_base_folder, exist_ok=True)\n",
    "    for rep_image, sim_images in similar_image_folders.items():\n",
    "        group_folder = os.path.join(output_base_folder, f\"similar_to_{os.path.splitext(rep_image)[0]}\")\n",
    "        os.makedirs(group_folder, exist_ok=True)\n",
    "        shutil.copy(os.path.join(image_folder_path, rep_image), os.path.join(group_folder, rep_image))\n",
    "        for sim_image in sim_images:\n",
    "            shutil.copy(os.path.join(image_folder_path, sim_image), os.path.join(group_folder, sim_image))\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    image_folder_path = \"P:\\\\CAD_Retrival\\\\Testing3\"\n",
    "    output_base_folder = \"P:\\\\CAD_Retrival\\\\Testing3(Output)\"\n",
    "    distance_threshold = 0.5\n",
    "    num_neighbors = 10\n",
    "\n",
    "    # Validate directory\n",
    "    if not os.path.exists(image_folder_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {image_folder_path}\")\n",
    "\n",
    "    # Model and transform\n",
    "    model = ResNet50Features().to(device)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Extract features\n",
    "    image_paths = [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path) if f.endswith('.png')]\n",
    "    print(f\"Found {len(image_paths)} PNG files in {image_folder_path}\")\n",
    "    features, image_names = extract_features_batch(image_paths, model, transform, device)\n",
    "    \n",
    "    # Validate features\n",
    "    features_np = features.numpy().astype('float32')\n",
    "    if np.any(~np.isfinite(features_np)):\n",
    "        print(\"Warning: Feature vectors contain NaN or infinite values.\")\n",
    "        features_np = features_np[np.isfinite(features_np).all(axis=1)]\n",
    "        image_names = [name for i, name in enumerate(image_names) if np.isfinite(features_np[i]).all()]\n",
    "    \n",
    "    # Build FAISS index\n",
    "    index = build_faiss_index(features_np)\n",
    "\n",
    "    # Find similar images\n",
    "    similar_image_folders = find_similar_images(index, features_np, image_names, k=num_neighbors, distance_threshold=distance_threshold)\n",
    "\n",
    "    # Copy to folders\n",
    "    copy_similar_images_to_folders(similar_image_folders, image_folder_path, output_base_folder)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\Order to PC\\CAD_Reconstruction\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "m:\\Order to PC\\CAD_Reconstruction\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1656/1656 [00:52<00:00, 31.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index...\n",
      "Organizing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1656 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts features from images using a pre-trained PyTorch model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='resnet50', pretrained=True, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.model = self.load_model(pretrained)\n",
    "        self.transform = self.get_transform()\n",
    "\n",
    "    def load_model(self, pretrained):\n",
    "        \"\"\"Loads a pre-trained model and sets it to evaluation mode.\"\"\"\n",
    "        model = getattr(models, self.model_name)(pretrained=pretrained)\n",
    "        # Remove the classification layer (fc or classifier) to get features\n",
    "        if self.model_name.startswith('resnet'):\n",
    "             modules = list(model.children())[:-1]  # Remove last fc layer\n",
    "        elif self.model_name.startswith('vgg'):\n",
    "             modules = list(model.features)  # Remove classifier\n",
    "        elif self.model_name.startswith('efficientnet'):\n",
    "            modules = list(model.features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model architecture: {self.model_name}\") #Add exception if model not supported\n",
    "\n",
    "        model = torch.nn.Sequential(*modules)\n",
    "        model = model.to(self.device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_transform(self):\n",
    "        \"\"\"Defines the image transformations.\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize to the model's input size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def extract_features(self, image_path):\n",
    "        \"\"\"Extracts features from a single image.\"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "        except (FileNotFoundError, OSError) as e:\n",
    "            print(f\"Error opening image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        img = self.transform(img)\n",
    "        img = img.unsqueeze(0)  # Add batch dimension (1 x C x H x W)\n",
    "        img = img.to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            features = self.model(img)\n",
    "            features = features.squeeze()  # Remove extra dimensions\n",
    "\n",
    "        return features.cpu().numpy()  # Move to CPU and convert to NumPy array\n",
    "\n",
    "\n",
    "\n",
    "class ImageSimilaritySearch:\n",
    "    \"\"\"\n",
    "    Uses FAISS to build an index and find similar images.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension):\n",
    "        self.dimension = dimension\n",
    "        #self.index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)\n",
    "        self.index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))  # Store image IDs\n",
    "\n",
    "    def build_index(self, features, ids):\n",
    "        \"\"\"Builds the FAISS index.\"\"\"\n",
    "        features = np.array(features).astype('float32')\n",
    "        ids = np.array(ids)\n",
    "        self.index.add_with_ids(features,ids) # add with ids\n",
    "\n",
    "    def search(self, query_features, k=10):\n",
    "        \"\"\"Searches for the k most similar images.\"\"\"\n",
    "        query_features = np.array(query_features).astype('float32').reshape(1, -1)  # Reshape for single query\n",
    "        distances, indices = self.index.search(query_features, k)\n",
    "        return distances, indices\n",
    "\n",
    "\n",
    "def organize_images(image_dir, output_dir, feature_extractor, similarity_search, threshold=10.0, k_neighbors=10):\n",
    "    \"\"\"\n",
    "    Finds similar images and moves them to subfolders.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    image_paths = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))] # consider different image formats\n",
    "    image_ids = list(range(len(image_paths))) #create a list of ids\n",
    "\n",
    "    # Extract features for all images\n",
    "    all_features = []\n",
    "    valid_image_ids = []  # Keep track of images with valid features\n",
    "    print(\"Extracting features...\")\n",
    "    for i, image_path in enumerate(tqdm(image_paths)):  # Use tqdm for progress\n",
    "        features = feature_extractor.extract_features(image_path)\n",
    "        if features is not None:  # Check if features were extracted\n",
    "            all_features.append(features)\n",
    "            valid_image_ids.append(image_ids[i])  # Use the original index\n",
    "\n",
    "    print(\"Building FAISS index...\")\n",
    "    similarity_search.build_index(all_features, valid_image_ids)\n",
    "\n",
    "    print(\"Organizing images...\")\n",
    "    processed_images = set()  # Keep track of images that have been moved\n",
    "    folder_counter = 0\n",
    "\n",
    "    for i, image_id in enumerate(tqdm(valid_image_ids)):\n",
    "        image_path = image_paths[image_id]  # Corrected index\n",
    "        if image_id in processed_images:\n",
    "            continue\n",
    "\n",
    "        # Create a new subfolder\n",
    "        folder_counter += 1\n",
    "        new_folder_path = os.path.join(output_dir, f\"group_{folder_counter}\")\n",
    "        os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "        # Move the current image to the new folder\n",
    "        shutil.move(image_path, os.path.join(new_folder_path, os.path.basename(image_path)))\n",
    "        processed_images.add(image_id)\n",
    "        # Find similar images\n",
    "        distances, indices = similarity_search.search(all_features[i], k=k_neighbors) # Pass the feature directly, the index already built\n",
    "\n",
    "        # Move similar images to the same folder\n",
    "        for j in range(indices.shape[1]):  # Iterate over the results\n",
    "             neighbor_id = indices[0][j] # Get the id from index result.  indices is an array of arrays\n",
    "             if neighbor_id != image_id and neighbor_id not in processed_images and distances[0][j] < threshold :\n",
    "                neighbor_path = image_paths[neighbor_id] # Get the correct path of image\n",
    "                try:\n",
    "                    shutil.move(neighbor_path, os.path.join(new_folder_path, os.path.basename(neighbor_path)))\n",
    "                    processed_images.add(neighbor_id)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Warning: Image file not found during move: {neighbor_path}\")\n",
    "                except Exception as e: #Catch other errors during move\n",
    "                    print(f\"Error moving image: {neighbor_path}, {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    image_dir = \"P:\\CAD_Retrival\\Testing3\"  # Replace with your image directory\n",
    "    output_dir = \"P:\\CAD_Retrival\\Testing3(Output)\"  # Replace with your desired output directory\n",
    "    model_name = 'resnet50'\n",
    "    threshold = 250.0   # Adjust this threshold based on your images and model.  EXPERIMENT!\n",
    "    k_neighbors= 20   # Number of neighbors considered, could affect your grouping\n",
    "\n",
    "    #Feature dimension extraction\n",
    "    if model_name.startswith('resnet'):\n",
    "        feature_dimension = 2048  # For ResNet50 (and other ResNets)\n",
    "    elif model_name.startswith('vgg'):\n",
    "        feature_dimension = 25088  # For VGG16/VGG19 (check features[-1].out_features)\n",
    "    elif model_name.startswith('efficientnet'):\n",
    "         feature_dimension = 1280 #For efficientnet_b0\n",
    "    else:\n",
    "         raise ValueError(f\"Unsupported model architecture: {model_name}\") #Add exception if model not supported\n",
    "\n",
    "\n",
    "    feature_extractor = ImageFeatureExtractor(model_name=model_name)\n",
    "    similarity_search = ImageSimilaritySearch(dimension=feature_dimension)\n",
    "    start_time = time.time()\n",
    "\n",
    "    organize_images(image_dir, output_dir, feature_extractor, similarity_search, threshold=threshold,k_neighbors=k_neighbors)\n",
    "    end_time = time.time()\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
